install sparkr

wget https://repo.anaconda.com/archive/Anaconda2-2019.10-Linux-x86_64.sh

sudo bash Anaconda2-2019.10-Linux-x86_64.sh
source .bashrc
which python
jupyter notebook --generate-config

mkdir certs
$ cd certs
$ sudo openssl req -x509 -nodes -days 365 -newkey rsa:1024 -keyout mycert.pem -out mycert.pem

cd ~/.jupyter/

vi jupyter_notebook_config.py

c = get_config()

# Notebook config this is where you saved your pem cert
c.NotebookApp.certfile = u'/home/ubuntu/certs/mycert.pem' 
# Run on all IP addresses of your instance
c.NotebookApp.ip = '*'
# Don't open browser by default
c.NotebookApp.open_browser = False  
# Fix port to 8888
c.NotebookApp.port = 8888


---------------------------------------
ubuntu

wget https://www.apache.org/dyn/closer.lua/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz

tar -xvf spark-3.0.0-bin-hadoop3.2.tgz


hadoop2

https://datawookie.netlify.app/blog/2017/07/installing-hadoop-on-ubuntu/


https://downloads.apache.org/hadoop/common/hadoop-3.2.1/hadoop-3.2.1.tar.gz

https://downloads.apache.org/hadoop/common/hadoop-3.2.1/hadoop-3.2.1.tar.gz

tar -xvf hadoop-3.2.1/hadoop-3.2.1.tar.gz

sudo mv hadoop-3.2.1 /usr/local/hadoop
readlink -f /usr/bin/java | sed "s#bin/java##"
/usr/lib/jvm/java-8-openjdk-amd64/jre/

 /usr/lib/jvm/java-8-openjdk-amd64/jre/
 
 sudo apt install default-jdk
 
 tar -xvf spark-3.0.0-bin-hadoop3.2.tgz
  sudo mv spark-3.0.0-bin-hadoop3.2 /usr/local/
  sudo ln -s /usr/local/spark-3.0.0-bin-hadoop3.2/ /usr/local/spark
  export SPARK_HOME=/usr/local/spark
  
  $SPARK_HOME/sbin/start-master.sh
  $SPARK_HOME/sbin/start-slave.sh spark://ethane:7077
  
  
  
  $SPARK_HOME/bin/spark-shell
  
  export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/hadoop/lib/native
 